{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ii3ihksojvA"
   },
   "source": [
    "# LangChain 1-hour Sprint\n",
    "\n",
    "![LangChain Logo](https://github.com/rastringer/promptcraft_notebooks/blob/main/images/langchain.png?raw=1)\n",
    "\n",
    "LangChain is a framework for developing applications infused with LLM magic. In this notebook, we will cover some of its most useful and fun features, including:\n",
    "\n",
    "* Templates\n",
    "* Memory\n",
    "* Working with APIs\n",
    "* Chains\n",
    "* Agents\n",
    "* Vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLvQkFXbTd2C"
   },
   "source": [
    "Let's start by importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A2k24lCDonBx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.60.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.61.0-py2.py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.32.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.5)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.17)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.1)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.7.4)\n",
      "Downloading google_cloud_aiplatform-1.61.0-py2.py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.60.0\n",
      "    Uninstalling google-cloud-aiplatform-1.60.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.60.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "google-cloud-pubsub 2.21.4 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.48.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.61.0 protobuf-3.20.3\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.0.64-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain)\n",
      "  Downloading langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.17)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting langchain-community<0.3.0,>=0.2.10 (from langchain-experimental)\n",
      "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "\u001b[33mWARNING: langchain 0.2.12 does not provide the extra 'docarray'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.10->langchain-experimental)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.10->langchain-experimental)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.10->langchain-experimental)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.10->langchain-experimental)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_experimental-0.0.64-py3-none-any.whl (204 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.29-py3-none-any.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.98-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, mypy-extensions, marshmallow, typing-inspect, langsmith, langchain-core, dataclasses-json, langchain-text-splitters, langchain, langchain-community, langchain-experimental\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-0.2.12 langchain-community-0.2.11 langchain-core-0.2.29 langchain-experimental-0.0.64 langchain-text-splitters-0.2.2 langsmith-0.1.98 marshmallow-3.21.3 mypy-extensions-1.0.0 tenacity-8.5.0 typing-inspect-0.9.0\n",
      "Collecting langchain-google-vertexai\n",
      "  Downloading langchain_google_vertexai-1.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (1.61.0)\n",
      "Collecting google-cloud-storage<3.0.0,>=2.17.0 (from langchain-google-vertexai)\n",
      "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.25 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (0.2.29)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.32.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (24.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.0.5)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.10.17)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.16)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai)\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2.4.1)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (1.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (0.1.98)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (4.12.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.63.1)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.48.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.48.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.12.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.25->langchain-google-vertexai) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai) (2024.7.4)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.6.0)\n",
      "Downloading langchain_google_vertexai-1.0.8-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-resumable-media, google-api-core, google-cloud-storage, langchain-google-vertexai\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 2.7.1\n",
      "    Uninstalling google-resumable-media-2.7.1:\n",
      "      Successfully uninstalled google-resumable-media-2.7.1\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.1\n",
      "    Uninstalling google-api-core-1.34.1:\n",
      "      Successfully uninstalled google-api-core-1.34.1\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.14.0\n",
      "    Uninstalling google-cloud-storage-2.14.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.19.1 which is incompatible.\n",
      "google-cloud-pubsub 2.21.4 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.11.1 google-cloud-storage-2.18.2 google-resumable-media-2.7.2 langchain-google-vertexai-1.0.8\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.3.1\n",
      "Collecting pydantic==1.10.8\n",
      "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.10.8) (4.12.2)\n",
      "Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.17\n",
      "    Uninstalling pydantic-1.10.17:\n",
      "      Successfully uninstalled pydantic-1.10.17\n",
      "Successfully installed pydantic-1.10.8\n",
      "Collecting chromadb==0.3.26\n",
      "  Downloading chromadb-0.3.26-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (2.32.3)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (1.10.8)\n",
      "Collecting hnswlib>=0.7 (from chromadb==0.3.26)\n",
      "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clickhouse-connect>=0.5.7 (from chromadb==0.3.26)\n",
      "  Downloading clickhouse_connect-0.7.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting duckdb>=0.7.1 (from chromadb==0.3.26)\n",
      "  Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (0.111.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.30.3)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (1.24.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.3.26)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (4.12.2)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb==0.3.26)\n",
      "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb==0.3.26)\n",
      "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb==0.3.26)\n",
      "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (7.7.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2024.7.4)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (1.26.19)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2024.1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (0.23.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (4.3.3)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (0.0.4)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (0.27.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (0.0.9)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (2.2.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.26)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (24.3.25)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (24.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (3.20.3)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb==0.3.26)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.26) (2.9.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.26) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.26) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.26)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.26)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.26) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.26) (3.7)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb==0.3.26)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.26) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.85.1->chromadb==0.3.26) (2.6.1)\n",
      "Requirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi>=0.85.1->chromadb==0.3.26) (0.12.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi>=0.85.1->chromadb==0.3.26) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi>=0.85.1->chromadb==0.3.26) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi>=0.85.1->chromadb==0.3.26) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.85.1->chromadb==0.3.26) (2.0.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.26)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb==0.3.26)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->fastapi>=0.85.1->chromadb==0.3.26) (1.2.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.85.1->chromadb==0.3.26) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.85.1->chromadb==0.3.26) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.85.1->chromadb==0.3.26) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.85.1->chromadb==0.3.26) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi>=0.85.1->chromadb==0.3.26) (0.1.2)\n",
      "Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading clickhouse_connect-0.7.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (972 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m972.9/972.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=200597 sha256=162ddc7c569dbeb06e32a5bd0bfc46346e9dcf68d1d0e8d5f5ddf66abd7d111a\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
      "Successfully built hnswlib\n",
      "Installing collected packages: mpmath, monotonic, sympy, pulsar-client, humanfriendly, hnswlib, duckdb, clickhouse-connect, backoff, posthog, huggingface-hub, coloredlogs, tokenizers, onnxruntime, chromadb\n",
      "Successfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.7.18 coloredlogs-15.0.1 duckdb-1.0.0 hnswlib-0.8.0 huggingface-hub-0.24.5 humanfriendly-10.0 monotonic-1.6 mpmath-1.3.0 onnxruntime-1.18.1 posthog-3.5.0 pulsar-client-3.5.0 sympy-1.13.1 tokenizers-0.20.0\n",
      "Collecting typing-inspect==0.8.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting typing_extensions==4.5.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect==0.8.0) (1.0.0)\n",
      "Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: typing_extensions, typing-inspect\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.9.0\n",
      "    Uninstalling typing-inspect-0.9.0:\n",
      "      Successfully uninstalled typing-inspect-0.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy 2.0.31 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "fastapi 0.111.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "langchain-core 0.2.29 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-inspect-0.8.0 typing_extensions-4.5.0\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m762.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.24.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.5.0)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.7.24)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, triton, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.0\n",
      "    Uninstalling tokenizers-0.20.0:\n",
      "      Successfully uninstalled tokenizers-0.20.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.4 sentence-transformers-3.0.1 tokenizers-0.19.1 torch-2.4.0 transformers-4.44.0 triton-3.0.0 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-cloud-aiplatform\n",
    "# LangChain\n",
    "! pip install langchain langchain-experimental langchain[docarray]\n",
    "! pip install -U langchain-google-vertexai\n",
    "! pip install pypdf\n",
    "! pip install pydantic==1.10.8\n",
    "# Open source vector store\n",
    "! pip install chromadb==0.3.26\n",
    "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n",
    "# For dense vector representations of text\n",
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OPxAFOghonE3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AGVFxl9fjt0I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N7Y7McwEz8Zk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.2.12\n",
      "Vertex AI SDK version: 1.61.0\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from pydantic import BaseModel\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LogO0tJ9z-73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723293223.946788    7462 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "# We will use chat for some tasks\n",
    "chat = ChatVertexAI(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True)\n",
    "\n",
    "# And we will use the general text llm for others\n",
    "llm = VertexAI(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by3p-b1sJl9C"
   },
   "source": [
    "The simplest LangChain use is to create chats comprising of a `SystemMessage` and `HumanMessage`. This is similar to the `context` and `user_message` that we provide the LLM using the Python client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lP39CLLa0F2I",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Hi there! How can I help you today?', response_metadata={'is_blocked': False, 'errors': (), 'safety_attributes': [{'Finance': 0.1, 'Health': 0.3, 'Insult': 0.1, 'Sexual': 0.1}], 'grounding_metadata': {'citations': [], 'search_queries': []}, 'usage_metadata': {'candidates_billable_characters': 28.0, 'candidates_token_count': 10.0, 'prompt_billable_characters': 5.0, 'prompt_token_count': 1.0}}, id='run-1b6f4417-9f7e-47c1-b028-446d90ef9267-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZdGjBV420Y6B",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kidney beans and tomatoes are a great combination for a quick and easy lunch. Here's a simple recipe you can try:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "- 1 can (15 ounces) kidney beans, rinsed and drained\n",
      "- 1 can (14.5 ounces) diced tomatoes, undrained\n",
      "- 1/2 cup chopped onion\n",
      "- 1/2 cup chopped green bell pepper\n",
      "- 1/4 cup chopped cilantro\n",
      "- 1 teaspoon chili powder\n",
      "- 1/2 teaspoon ground cumin\n",
      "- 1/4 teaspoon salt\n",
      "- 1/4 teaspoon black pepper\n",
      "- 1/4 cup shredded cheese (optional)\n",
      "- Tortillas, chips, or bread, for serving\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a large skillet, heat a little bit of oil over medium heat.\n",
      "2. Add the onion and green bell pepper and cook until softened, about 5 minutes.\n",
      "3. Add the kidney beans, tomatoes, chili powder, cumin, salt, and black pepper. Bring to a simmer and cook for 10 minutes, or until the beans are heated through.\n",
      "4. Stir in the cilantro and cheese, if using.\n",
      "5. Serve with tortillas, chips, or bread.\n",
      "\n",
      "Enjoy your delicious and easy kidney bean and tomato lunch!\n"
     ]
    }
   ],
   "source": [
    "res = chat(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are an expert chef that thinks of imaginative recipies when people give you ingredients.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"I have some kidney beans and tomatoes, what would be an easy lunch?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiNioRJBeCeU"
   },
   "source": [
    "## Prompt templates\n",
    "\n",
    "Templates are an abstraction that can help keep prompts modular and reusable. This can be especially important in large applications which may require long and varied prompts.\n",
    "\n",
    "Templates may include few-short examples, instructions, or context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SSkWubxs1CGM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The template_string parameters sets the context for the ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cIlbbFVA0agh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upPmDaJTLgw7"
   },
   "source": [
    "The chat prompts are envisioned as a series of messages. Notice the `.messages` and `format_messages` methods in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Yb3dCKwLq2V",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the template\n",
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mUeNeCwA1BiE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check just the prompt\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dRYnKpuG0-Zz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpful method to keep track of a template's inputs\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaD1QsBiKePJ"
   },
   "source": [
    "In this simple example, we translate a customer e-mail into phonetic Glaswegian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9RzYUX3o1G-D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator_style = \"\"\"A translator that writes in phonetic Glaswegian.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VOYeAk6i1Kky",
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "This smashing little coffee maker is simply brilliant! \\\n",
    "I'm so pleased with how easy it is to use and how quickly it brews \\\n",
    "a cracking cup of coffee. \\\n",
    "I'm over the moon with this purchase and would highly recommend it \\\n",
    "to any other coffee lover looking for a top-notch brew every time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mwuKkrjL1MP8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The format_messages method sets up the task specified in the template\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=translator_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lTqCIvvh1PSz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aww, right, so, listen up, pal. This wee coffee maker is pure dead brilliant! It's a doddle tae use, and it makes a braw cuppa in nae time. Ah'm ower the moon wi' it, and Ah'd recommend it tae ony coffee lover lookin' fur a wee belter.\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE_nyfIgmKO6"
   },
   "source": [
    "## Parsing outputs\n",
    "\n",
    "LangChain makes it easy to return objects from the LLM in a format which we can use for further tasks (for example, adding an item of interest to a shopping cart, or providing a short list back to the LLM for additional questions).\n",
    "\n",
    "Here is an example of parsing customer reviews of a three-course meal in a restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i-karEJ_mtBl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "The excellent barbecue cauliflower starter left \\\n",
    "a lasting impression -- gorgeous presentation and flavors, really geared the tastebuds into action. \\\n",
    "Moving on to the main course, pretty great also. \\\n",
    "Delicious and flavorful chickpea and vegetable curry. They really nailed the buttery consistency, \\\n",
    "depth and balance of the spices. \\\n",
    "The dessert was a bit bland. I opted for a vegan chocolate mousse, \\\n",
    "hoping for a decadent and indulgent finale to my meal. \\\n",
    "It was very visually appealing but was missing the smooth, velvety \\\n",
    "texture of a great mousse.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the input text, extract the following details: \\\n",
    "starter: How did the reviewer find the first course? \\\n",
    "Rate either Poor, Good, or Excellent. \\\n",
    "Do the same for the main course and dessert\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "starter\n",
    "main_course\n",
    "dessert\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Cp1dMFxkYd5H",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the input text, extract the following details: starter: How did the reviewer find the first course? Rate either Poor, Good, or Excellent. Do the same for the main course and dessert\\n\\nFormat the output as JSON with the following keys:\\nstarter\\nmain_course\\ndessert\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jHoTT2hrXzRj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "  \"starter\": \"Excellent\",\n",
      "  \"main_course\": \"Good\",\n",
      "  \"dessert\": \"Poor\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = chat(messages, temperature=0.1)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV8VHzteZsfG"
   },
   "source": [
    "Though it looks like JSON, our output is actually a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fT7RLVX9ZYeg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jr6xxJ2ZxQr"
   },
   "source": [
    "This means we are unable to access values in this fashion (will result in an error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "16sEr4XhZjHS",
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_course\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "response.content.get(\"main_course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YwKaaROZ1xa"
   },
   "source": [
    "This is where LangChain's parser comes in. Here, we import the `ResponseSchema` and `StructuredOutputParser`, which we use to define the format of the results from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "u1IPczDvZrJp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "starter_schema = ResponseSchema(name=\"starter\", description=\"Review of the starter\")\n",
    "main_course_schema = ResponseSchema(name=\"main_course\", description=\"Review of the main course\")\n",
    "dessert_schema = ResponseSchema(name=\"dessert\", description=\"Review of the dessert\")\n",
    "\n",
    "response_schemas = [starter_schema, main_course_schema, dessert_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tfkKmhhDb5K2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b4ztjoM8aXF5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"starter\": string  // Review of the starter\n",
      "\t\"main_course\": string  // Review of the main course\n",
      "\t\"dessert\": string  // Review of the dessert\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpoJt9EpcfKN"
   },
   "source": [
    "Now we can update our prior review template to include the format instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qeh2oItsa5Lu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the input text, extract the following details: \\\n",
    "starter: How did the reviewer find the first course? \\\n",
    "Rate either Poor, Good, or Excellent. \\\n",
    "Do the same for the main course and dessert\n",
    "\n",
    "starter\n",
    "main_course\n",
    "dessert\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review,\n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "O779zcYwYHwV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the input text, extract the following details: starter: How did the reviewer find the first course? Rate either Poor, Good, or Excellent. Do the same for the main course and dessert\n",
      "\n",
      "starter\n",
      "main_course\n",
      "dessert\n",
      "\n",
      "text: The excellent barbecue cauliflower starter left a lasting impression -- gorgeous presentation and flavors, really geared the tastebuds into action. Moving on to the main course, pretty great also. Delicious and flavorful chickpea and vegetable curry. They really nailed the buttery consistency, depth and balance of the spices. The dessert was a bit bland. I opted for a vegan chocolate mousse, hoping for a decadent and indulgent finale to my meal. It was very visually appealing but was missing the smooth, velvety texture of a great mousse.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"starter\": string  // Review of the starter\n",
      "\t\"main_course\": string  // Review of the main course\n",
      "\t\"dessert\": string  // Review of the dessert\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rzBUsr3WYK5z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4nq3mM8crLn"
   },
   "source": [
    "Let's try it on the same review\n",
    "\n",
    "Our response starts as an `AIMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "a4EVPQcQcx-_",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbjFhXIXCGZb"
   },
   "source": [
    "Here we parse the `AIMessage` into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UslUSpVTczoX",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'starter': 'Excellent', 'main_course': 'Good', 'dessert': 'Poor'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-g9ngq2DbUJ"
   },
   "source": [
    "Thanks to LangChain's parser, we now have a Python dictionary which we can use for further tasks, for example taking part of the response and using it as an input to another function / process etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FWTgsp_rdGq4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bkEz820ndLGJ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get(\"main_course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MlLBtQI-lCX"
   },
   "source": [
    "## API chains\n",
    "\n",
    "Another of LangChain's useful features is the ability to call external APIs within chains.\n",
    "\n",
    "In this example, we use the `open-meteo.com` API to get weather reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kCOhnssF-tvb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://api.open-meteo.com/v1/forecast?latitude=55.9533&longitude=-3.1883&hourly=temperature_2m&temperature_unit=fahrenheit&current_weather=true\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"latitude\":55.96,\"longitude\":-3.18,\"generationtime_ms\":0.05900859832763672,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":69.0,\"current_weather_units\":{\"time\":\"iso8601\",\"interval\":\"seconds\",\"temperature\":\"°F\",\"windspeed\":\"km/h\",\"winddirection\":\"°\",\"is_day\":\"\",\"weathercode\":\"wmo code\"},\"current_weather\":{\"time\":\"2024-08-10T12:30\",\"interval\":900,\"temperature\":66.3,\"windspeed\":26.7,\"winddirection\":260,\"is_day\":1,\"weathercode\":1},\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°F\"},\"hourly\":{\"time\":[\"2024-08-10T00:00\",\"2024-08-10T01:00\",\"2024-08-10T02:00\",\"2024-08-10T03:00\",\"2024-08-10T04:00\",\"2024-08-10T05:00\",\"2024-08-10T06:00\",\"2024-08-10T07:00\",\"2024-08-10T08:00\",\"2024-08-10T09:00\",\"2024-08-10T10:00\",\"2024-08-10T11:00\",\"2024-08-10T12:00\",\"2024-08-10T13:00\",\"2024-08-10T14:00\",\"2024-08-10T15:00\",\"2024-08-10T16:00\",\"2024-08-10T17:00\",\"2024-08-10T18:00\",\"2024-08-10T19:00\",\"2024-08-10T20:00\",\"2024-08-10T21:00\",\"2024-08-10T22:00\",\"2024-08-10T23:00\",\"2024-08-11T00:00\",\"2024-08-11T01:00\",\"2024-08-11T02:00\",\"2024-08-11T03:00\",\"2024-08-11T04:00\",\"2024-08-11T05:00\",\"2024-08-11T06:00\",\"2024-08-11T07:00\",\"2024-08-11T08:00\",\"2024-08-11T09:00\",\"2024-08-11T10:00\",\"2024-08-11T11:00\",\"2024-08-11T12:00\",\"2024-08-11T13:00\",\"2024-08-11T14:00\",\"2024-08-11T15:00\",\"2024-08-11T16:00\",\"2024-08-11T17:00\",\"2024-08-11T18:00\",\"2024-08-11T19:00\",\"2024-08-11T20:00\",\"2024-08-11T21:00\",\"2024-08-11T22:00\",\"2024-08-11T23:00\",\"2024-08-12T00:00\",\"2024-08-12T01:00\",\"2024-08-12T02:00\",\"2024-08-12T03:00\",\"2024-08-12T04:00\",\"2024-08-12T05:00\",\"2024-08-12T06:00\",\"2024-08-12T07:00\",\"2024-08-12T08:00\",\"2024-08-12T09:00\",\"2024-08-12T10:00\",\"2024-08-12T11:00\",\"2024-08-12T12:00\",\"2024-08-12T13:00\",\"2024-08-12T14:00\",\"2024-08-12T15:00\",\"2024-08-12T16:00\",\"2024-08-12T17:00\",\"2024-08-12T18:00\",\"2024-08-12T19:00\",\"2024-08-12T20:00\",\"2024-08-12T21:00\",\"2024-08-12T22:00\",\"2024-08-12T23:00\",\"2024-08-13T00:00\",\"2024-08-13T01:00\",\"2024-08-13T02:00\",\"2024-08-13T03:00\",\"2024-08-13T04:00\",\"2024-08-13T05:00\",\"2024-08-13T06:00\",\"2024-08-13T07:00\",\"2024-08-13T08:00\",\"2024-08-13T09:00\",\"2024-08-13T10:00\",\"2024-08-13T11:00\",\"2024-08-13T12:00\",\"2024-08-13T13:00\",\"2024-08-13T14:00\",\"2024-08-13T15:00\",\"2024-08-13T16:00\",\"2024-08-13T17:00\",\"2024-08-13T18:00\",\"2024-08-13T19:00\",\"2024-08-13T20:00\",\"2024-08-13T21:00\",\"2024-08-13T22:00\",\"2024-08-13T23:00\",\"2024-08-14T00:00\",\"2024-08-14T01:00\",\"2024-08-14T02:00\",\"2024-08-14T03:00\",\"2024-08-14T04:00\",\"2024-08-14T05:00\",\"2024-08-14T06:00\",\"2024-08-14T07:00\",\"2024-08-14T08:00\",\"2024-08-14T09:00\",\"2024-08-14T10:00\",\"2024-08-14T11:00\",\"2024-08-14T12:00\",\"2024-08-14T13:00\",\"2024-08-14T14:00\",\"2024-08-14T15:00\",\"2024-08-14T16:00\",\"2024-08-14T17:00\",\"2024-08-14T18:00\",\"2024-08-14T19:00\",\"2024-08-14T20:00\",\"2024-08-14T21:00\",\"2024-08-14T22:00\",\"2024-08-14T23:00\",\"2024-08-15T00:00\",\"2024-08-15T01:00\",\"2024-08-15T02:00\",\"2024-08-15T03:00\",\"2024-08-15T04:00\",\"2024-08-15T05:00\",\"2024-08-15T06:00\",\"2024-08-15T07:00\",\"2024-08-15T08:00\",\"2024-08-15T09:00\",\"2024-08-15T10:00\",\"2024-08-15T11:00\",\"2024-08-15T12:00\",\"2024-08-15T13:00\",\"2024-08-15T14:00\",\"2024-08-15T15:00\",\"2024-08-15T16:00\",\"2024-08-15T17:00\",\"2024-08-15T18:00\",\"2024-08-15T19:00\",\"2024-08-15T20:00\",\"2024-08-15T21:00\",\"2024-08-15T22:00\",\"2024-08-15T23:00\",\"2024-08-16T00:00\",\"2024-08-16T01:00\",\"2024-08-16T02:00\",\"2024-08-16T03:00\",\"2024-08-16T04:00\",\"2024-08-16T05:00\",\"2024-08-16T06:00\",\"2024-08-16T07:00\",\"2024-08-16T08:00\",\"2024-08-16T09:00\",\"2024-08-16T10:00\",\"2024-08-16T11:00\",\"2024-08-16T12:00\",\"2024-08-16T13:00\",\"2024-08-16T14:00\",\"2024-08-16T15:00\",\"2024-08-16T16:00\",\"2024-08-16T17:00\",\"2024-08-16T18:00\",\"2024-08-16T19:00\",\"2024-08-16T20:00\",\"2024-08-16T21:00\",\"2024-08-16T22:00\",\"2024-08-16T23:00\"],\"temperature_2m\":[57.7,57.6,57.4,57.4,57.3,57.3,57.8,59.4,60.8,62.9,64.2,65.1,65.9,66.6,67.0,66.2,65.6,64.5,62.9,61.2,59.4,58.0,56.7,55.9,55.2,54.7,54.1,53.7,53.3,53.1,54.0,56.3,58.8,60.9,63.0,64.9,67.1,68.6,67.2,65.7,64.2,63.0,62.5,63.8,62.6,61.0,59.6,58.6,58.4,58.8,59.4,59.9,60.0,59.1,58.4,58.6,60.1,62.2,66.3,68.1,68.3,65.7,64.7,65.7,65.8,64.8,64.3,64.3,64.1,62.9,61.1,60.3,59.5,59.2,57.9,56.6,55.9,55.5,56.0,59.4,63.2,66.1,66.9,68.0,69.0,69.5,69.8,69.5,68.3,66.4,64.7,63.8,62.9,62.1,61.1,60.0,59.1,58.5,58.3,58.1,57.6,57.3,57.6,59.3,61.8,64.1,66.5,68.9,70.1,69.2,67.1,65.2,64.2,63.3,62.0,59.8,57.1,55.0,54.0,53.6,53.4,53.4,53.7,53.9,53.9,54.0,54.0,53.7,55.6,57.4,58.9,60.1,60.8,60.0,58.4,57.6,58.3,59.8,60.7,60.5,59.8,58.7,56.9,54.7,53.1,52.5,52.4,52.4,52.2,52.2,52.7,54.0,55.8,57.5,59.0,60.2,61.2,61.8,61.9,61.7,61.1,60.3,59.2,57.7,55.9,54.6,54.0,53.9]}}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The current temperature in Edinburgh, Scotland is 66.3 degrees Fahrenheit.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "\n",
    "llm = VertexAI(temperature=0)\n",
    "chain = APIChain.from_llm_and_api_docs(\n",
    "    llm,\n",
    "    open_meteo_docs.OPEN_METEO_DOCS,\n",
    "    verbose=True,\n",
    "    limit_to_domains=[\"https://api.open-meteo.com/\"],\n",
    ")\n",
    "chain.run(\n",
    "    \"How is the weather today in Edinburgh, Scotland, in Fahrenheit?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMao6bOb_vjj"
   },
   "source": [
    "### Wikipedia\n",
    "\n",
    "We can combine the Wikipedia pip package and LangChain's Wikipedia API wrapper get query results from the encyclopedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SaOqZrSk-y24",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723293375.797769    7462 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=39acb4e749208a5fd7d311e511ba0fadb942e0489bfaa7803487a1f7e6a94d8a\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hDASabouAEGS",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: House sparrow\\nSummary: The house sparrow (Passer domesticus) is a bird of the sparrow family Passeridae, found in most parts of the world. It is a small bird that has a typical length of 16 cm (6.3 in) and a mass of 24–39.5 g (0.85–1.39 oz). Females and young birds are coloured pale brown and grey, and males have brighter black, white, and brown markings. One of about 25 species in the genus Passer, the house sparrow is native to most of Europe, the Mediterranean Basin, and a large part of Asia. Its intentional or accidental introductions to many regions, including parts of Australasia, Africa, and the Americas, make it the most widely distributed wild bird.\\nThe house sparrow is strongly associated with human habitation, and can live in urban or rural settings. Though found in widely varied habitats and climates, it typically avoids extensive woodlands, grasslands, polar regions, and hot, dry deserts far away from human development. For sustenance, the house sparrow routinely feeds at home and public bird feeding stations, but naturally feeds on the seeds of grains, flowering plants and weeds. However, it is an opportunistic, omnivorous eater, and commonly catches insects, their larvae, caterpillars, invertebrates and many other natural foods.\\nBecause of its numbers, ubiquity, and association with human settlements, the house sparrow is culturally prominent. It is extensively, and usually unsuccessfully, persecuted as an agricultural pest. It has also often been kept as a pet, as well as being a food item and a symbol of lust, sexual potency, commonness, and vulgarity. Though it is widespread and abundant, its numbers have declined in some areas. The animal\\'s conservation status is listed as least concern on the IUCN Red List.\\n\\nPage: Grasshopper sparrow\\nSummary: The grasshopper sparrow (Ammodramus savannarum) is a small New World sparrow. It belongs to the genus Ammodramus, which contains three species that inhabit grasslands and prairies. Grasshopper sparrows are sometimes found in crop fields and they will readily colonize reclaimed grassland. In the core of their range, grasshopper sparrows are dependent upon large areas of grassland where they avoid trees and shrubs. They seek out heterogenous patches of prairie that contain clumps of dead grass or other vegetation where they conceal their nest, and also contain barer ground where they forage for insects (especially grasshoppers), spiders, and seeds. Grasshopper sparrows are unusual among New World sparrows in that they sing two distinct song types, the prevalence of which varies with the nesting cycle. The primary male song, a high trill preceded by a stereotyped series of short chips, is reminiscent of the sounds of grasshoppers and is the origin of this species\\' name. Like some other birds of the central North American grasslands, this species also moves around a lot, not only via annual migrations, but individuals frequently disperse between breeding attempts or breeding seasons. Grasshopper sparrows are in steep decline across their range, even in the core of the breeding distribution in the tallgrass prairies of the central Great Plains. The Florida grasshopper sparrow (Ammodramus savannarum floridanus) is highly endangered.\\n\\nPage: Spanish sparrow\\nSummary: The Spanish sparrow or willow sparrow (Passer hispaniolensis) is a passerine bird of the sparrow family Passeridae. It is found in the Mediterranean region and south-west and central Asia. It is very similar to the closely related house sparrow, and the two species show their close relation in a \"biological mix-up\" of hybridisation in the Mediterranean region, which complicates the taxonomy of this species.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "wikipedia.run(\"To which bird family does the field sparrow belong?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2g875ERCizg"
   },
   "source": [
    "### Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Etx0j9SCAsAM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the official languages in Turkmenistan, and their alphabets?',\n",
       " 'url': 'https://www.google.com/search?q=What+are+the+official+languages+in+Turkmenistan,+and+their+alphabets?',\n",
       " 'output': ' Extracted: The official language of Turkmenistan is Turkmen, spoken by 77% of the population. The Turkmen language has been written with a version of the Latin alphabet based on Turkish since Turkmenistan declared independence in 1991.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "template = \"\"\"Between >>> and <<< are the raw search result text from google.\n",
    "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
    "Use the format\n",
    "Extracted:<answer or \"not found\">\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"requests_result\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMRequestsChain(llm_chain=LLMChain(llm=VertexAI(temperature=0), prompt=PROMPT))\n",
    "question = \"What are the official languages in Turkmenistan, and their alphabets?\"\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
    "}\n",
    "chain(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3ed1-PQJYso"
   },
   "source": [
    "## Memory\n",
    "\n",
    "It is essential that LLMs keep some memory of the prior interactions in a chat to better inform their answers.\n",
    "\n",
    "LangChain offers several approaches and features in this regard. For all details, see the [Memory](https://python.langchain.com/docs/modules/memory/) section of the documentation.\n",
    "\n",
    "### ConversationBufferWindowMemory\n",
    "\n",
    "Maintains a list of the interactions of the conversation over time, using the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "a4eQhiiYJbQV",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi\\nAI: How are you?\\nHuman: Fine thanks\\nAI: Great'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"How are you?\"})\n",
    "memory.save_context({\"input\": \"Fine thanks\"},\n",
    "                    {\"output\": \"Great\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN3o4bWVRK23"
   },
   "source": [
    "### ConversationTokenBufferMemory\n",
    "\n",
    "This feature instead keeps a buffer of recent interactions in memory based on token length,  rather than number of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gvlXfMKXRZwO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"All alone, she dreams of the stars!\"},\n",
    "                    {\"output\": \"As she should!\"})\n",
    "memory.save_context({\"input\": \"Baking cookies today?\"},\n",
    "                    {\"output\": \"Behold the cookies!\"})\n",
    "memory.save_context({\"input\": \"Chatbots everywhere?\"},\n",
    "                    {\"output\": \"Certainly!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5GwhfKvvRc44",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: All alone, she dreams of the stars!\\nAI: As she should!\\nHuman: Baking cookies today?\\nAI: Behold the cookies!\\nHuman: Chatbots everywhere?\\nAI: Certainly!'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nj3OPlKRled"
   },
   "source": [
    "### Conversation summaries\n",
    "\n",
    "LangChain carries forward summaries of chat messages and flushes memory after a specified number of interactions or tokens.\n",
    "\n",
    "Let's first look at using the former, `ConversationBufferWindowMemory`.\n",
    "\n",
    "We set `verbose=True` to show the prompts and information carried forward by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pfSL0M7LPWE3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: My favourite sport is fencing. Any tips for how I can go pro?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Absolutely, here are some tips to help you become a professional fencer:\\n\\n**1. Master the basics:**\\n   - Ensure you have a solid understanding of the fundamental techniques, including footwork, bladework, and body positioning.\\n\\n**2. Physical fitness:**\\n   - Develop your overall fitness, including cardiovascular endurance, strength, and flexibility. Fencing requires quick movements and agility.\\n\\n**3. Practice regularly:**\\n   - Consistent practice is crucial. Dedicate time each day to refine your skills and techniques.\\n\\n**4. Join a fencing club:**\\n   - Training with experienced fencers can provide valuable insights and'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=VertexAI(temperature=0),\n",
    "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"My favourite sport is fencing. Any tips for how I can go pro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "xo2hMT-wPmuQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My favourite sport is fencing. Any tips for how I can go pro?\n",
      "AI:  Absolutely, here are some tips to help you become a professional fencer:\n",
      "\n",
      "**1. Master the basics:**\n",
      "   - Ensure you have a solid understanding of the fundamental techniques, including footwork, bladework, and body positioning.\n",
      "\n",
      "**2. Physical fitness:**\n",
      "   - Develop your overall fitness, including cardiovascular endurance, strength, and flexibility. Fencing requires quick movements and agility.\n",
      "\n",
      "**3. Practice regularly:**\n",
      "   - Consistent practice is crucial. Dedicate time each day to refine your skills and techniques.\n",
      "\n",
      "**4. Join a fencing club:**\n",
      "   - Training with experienced fencers can provide valuable insights and\n",
      "Human: What equipment do I need?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Here's a list of essential equipment you'll need for fencing:\\n\\n**1. Fencing weapon:**\\n   - Choose a weapon based on your preference and style. The three main types are foil, épée, and saber.\\n\\n**2. Fencing mask:**\\n   - A high-quality mask is crucial for protecting your face during bouts.\\n\\n**3. Fencing jacket:**\\n   - The jacket provides protection for your torso and arms.\\n\\n**4. Fencing knickers:**\\n   - These specialized pants offer flexibility and protection for your legs.\\n\\n**5. Fencing glove:**\\n   - A well-fitting\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"What equipment do I need?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ih9CLZ8AQOY0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My favourite sport is fencing. Any tips for how I can go pro?\n",
      "AI:  Absolutely, here are some tips to help you become a professional fencer:\n",
      "\n",
      "**1. Master the basics:**\n",
      "   - Ensure you have a solid understanding of the fundamental techniques, including footwork, bladework, and body positioning.\n",
      "\n",
      "**2. Physical fitness:**\n",
      "   - Develop your overall fitness, including cardiovascular endurance, strength, and flexibility. Fencing requires quick movements and agility.\n",
      "\n",
      "**3. Practice regularly:**\n",
      "   - Consistent practice is crucial. Dedicate time each day to refine your skills and techniques.\n",
      "\n",
      "**4. Join a fencing club:**\n",
      "   - Training with experienced fencers can provide valuable insights and\n",
      "Human: What equipment do I need?\n",
      "AI:  Here's a list of essential equipment you'll need for fencing:\n",
      "\n",
      "**1. Fencing weapon:**\n",
      "   - Choose a weapon based on your preference and style. The three main types are foil, épée, and saber.\n",
      "\n",
      "**2. Fencing mask:**\n",
      "   - A high-quality mask is crucial for protecting your face during bouts.\n",
      "\n",
      "**3. Fencing jacket:**\n",
      "   - The jacket provides protection for your torso and arms.\n",
      "\n",
      "**4. Fencing knickers:**\n",
      "   - These specialized pants offer flexibility and protection for your legs.\n",
      "\n",
      "**5. Fencing glove:**\n",
      "   - A well-fitting\n",
      "Human: Who are the greats of the sport I can emulate?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Here are some of the greatest fencers you can emulate to improve your skills and knowledge of the sport:\\n\\n**1. Valentina Vezzali (Italy):**\\n   - One of the most decorated fencers in history, with multiple Olympic and World Championship titles in women's foil.\\n\\n**2. Alexander Romankov (Russia):**\\n   - A legendary fencer known for his exceptional technique and achievements in men's foil.\\n\\n**3. Ed Zver (Russia):**\\n   - A former Olympic champion and World Champion in men's saber, renowned for his speed and precision.\\n\\n**4\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Who are the greats of the sport I can emulate?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "8elpkCMvP4qI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What equipment do I need?\n",
      "AI:  Here's a list of essential equipment you'll need for fencing:\n",
      "\n",
      "**1. Fencing weapon:**\n",
      "   - Choose a weapon based on your preference and style. The three main types are foil, épée, and saber.\n",
      "\n",
      "**2. Fencing mask:**\n",
      "   - A high-quality mask is crucial for protecting your face during bouts.\n",
      "\n",
      "**3. Fencing jacket:**\n",
      "   - The jacket provides protection for your torso and arms.\n",
      "\n",
      "**4. Fencing knickers:**\n",
      "   - These specialized pants offer flexibility and protection for your legs.\n",
      "\n",
      "**5. Fencing glove:**\n",
      "   - A well-fitting\n",
      "Human: Who are the greats of the sport I can emulate?\n",
      "AI:  Here are some of the greatest fencers you can emulate to improve your skills and knowledge of the sport:\n",
      "\n",
      "**1. Valentina Vezzali (Italy):**\n",
      "   - One of the most decorated fencers in history, with multiple Olympic and World Championship titles in women's foil.\n",
      "\n",
      "**2. Alexander Romankov (Russia):**\n",
      "   - A legendary fencer known for his exceptional technique and achievements in men's foil.\n",
      "\n",
      "**3. Ed Zver (Russia):**\n",
      "   - A former Olympic champion and World Champion in men's saber, renowned for his speed and precision.\n",
      "\n",
      "**4\n",
      "Human: What is my favourite sport?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I cannot answer that as the information is not available in the context.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we have now passed k=2, the LLM will be unable to answer\n",
    "conversation_with_summary.predict(input=\"What is my favourite sport?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z_Dhb87R_6q"
   },
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "Ensures conversational memory up to a specified token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Bp56IJU2Q6Ks",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, how are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I am doing well, thank you for asking. How can I help you today?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # Change max_token_limit here after running through the conversation\n",
    "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=600),\n",
    "    verbose=True,\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "AHnnO3MySR6R",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, how are you?\n",
      "AI:  I am doing well, thank you for asking. How can I help you today?\n",
      "Human: I'm learning the Rust programming language\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! Rust is a powerful and versatile programming language that emphasizes safety, performance, and concurrency. It's particularly well-suited for building systems software, embedded systems, and high-performance applications. If you have any specific questions about Rust, feel free to ask.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"I'm learning the Rust programming language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "f7IUBteTSKFo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, how are you?\n",
      "AI:  I am doing well, thank you for asking. How can I help you today?\n",
      "Human: I'm learning the Rust programming language\n",
      "AI:  That's great! Rust is a powerful and versatile programming language that emphasizes safety, performance, and concurrency. It's particularly well-suited for building systems software, embedded systems, and high-performance applications. If you have any specific questions about Rust, feel free to ask.\n",
      "Human: What's the best book to help me?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" There are several excellent books that can help you learn Rust. Here are a few highly recommended options:\\n\\n* **The Rust Programming Language** by Steve Klabnik and Carol Nichols: This is the official book from the Rust team and provides a comprehensive introduction to the language. It covers everything from basic syntax to advanced topics like memory management and concurrency.\\n\\n* **Rust in Action** by Tim McNamara: This book takes a more hands-on approach, with plenty of code examples and exercises. It's a great choice if you want to learn by doing.\\n\\n* **Programming Rust** by Jim Blandy: This book is known\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"What's the best book to help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "hFy53AIISZGp",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, how are you?\n",
      "AI:  I am doing well, thank you for asking. How can I help you today?\n",
      "Human: I'm learning the Rust programming language\n",
      "AI:  That's great! Rust is a powerful and versatile programming language that emphasizes safety, performance, and concurrency. It's particularly well-suited for building systems software, embedded systems, and high-performance applications. If you have any specific questions about Rust, feel free to ask.\n",
      "Human: What's the best book to help me?\n",
      "AI:  There are several excellent books that can help you learn Rust. Here are a few highly recommended options:\n",
      "\n",
      "* **The Rust Programming Language** by Steve Klabnik and Carol Nichols: This is the official book from the Rust team and provides a comprehensive introduction to the language. It covers everything from basic syntax to advanced topics like memory management and concurrency.\n",
      "\n",
      "* **Rust in Action** by Tim McNamara: This book takes a more hands-on approach, with plenty of code examples and exercises. It's a great choice if you want to learn by doing.\n",
      "\n",
      "* **Programming Rust** by Jim Blandy: This book is known\n",
      "Human: Wish me luck!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good luck with your Rust learning journey! Rust is a fantastic language, and I'm confident you'll find it both enjoyable and rewarding. If you have any further questions or need additional assistance, feel free to ask. I'm here to help you succeed.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the buffer here is updated and clears the earlier exchanges\n",
    "# Depending on how chatty the LLM is feeling, the token limit may have\n",
    "# already been reached, and this cell will yield a generic response.\n",
    "conversation_with_summary.predict(input=\"Wish me luck!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTc4s_mTS5r0"
   },
   "source": [
    "The following cell should generate a reply that is clearly restricted to the general benefits of learning Haskell and missing the previous context of someone trying to learn Rust.\n",
    "\n",
    "Run this cell, then go back to the Keep the conversation going with summaries cell and change `max_token_limit` to 700. Then re-run the entire conversation and notice how the model relates its ouptut about learning Haskell to the context of someone trying to learn Rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "dFg9ajZgSihj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, how are you?\n",
      "AI:  I am doing well, thank you for asking. How can I help you today?\n",
      "Human: I'm learning the Rust programming language\n",
      "AI:  That's great! Rust is a powerful and versatile programming language that emphasizes safety, performance, and concurrency. It's particularly well-suited for building systems software, embedded systems, and high-performance applications. If you have any specific questions about Rust, feel free to ask.\n",
      "Human: What's the best book to help me?\n",
      "AI:  There are several excellent books that can help you learn Rust. Here are a few highly recommended options:\n",
      "\n",
      "* **The Rust Programming Language** by Steve Klabnik and Carol Nichols: This is the official book from the Rust team and provides a comprehensive introduction to the language. It covers everything from basic syntax to advanced topics like memory management and concurrency.\n",
      "\n",
      "* **Rust in Action** by Tim McNamara: This book takes a more hands-on approach, with plenty of code examples and exercises. It's a great choice if you want to learn by doing.\n",
      "\n",
      "* **Programming Rust** by Jim Blandy: This book is known\n",
      "Human: Wish me luck!\n",
      "AI:  Good luck with your Rust learning journey! Rust is a fantastic language, and I'm confident you'll find it both enjoyable and rewarding. If you have any further questions or need additional assistance, feel free to ask. I'm here to help you succeed.\n",
      "Human: Would knowing Haskell help me?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes, knowing Haskell can be beneficial for learning Rust. Haskell is a purely functional programming language that emphasizes immutability, referential transparency, and lazy evaluation. While Rust is not a purely functional language, it does incorporate some functional programming concepts, such as immutability and pattern matching. Additionally, both Rust and Haskell place a strong emphasis on type safety and memory management. Therefore, having a background in Haskell can provide you with a solid foundation for understanding and appreciating these aspects of Rust.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Would knowing Haskell help me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipq1hc68ii_T"
   },
   "source": [
    "## Chains\n",
    "\n",
    "Complex applications will require chaining LLMs together, or with other components.\n",
    "\n",
    "We will cover the following types of chains:\n",
    "\n",
    "**Sequential chains**\n",
    "\n",
    "**Router chains**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUcwWDdmi0O1"
   },
   "source": [
    "### LLMChain\n",
    "\n",
    "An LLMChain simply provides a prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-rProHX1izYx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wAavEr0Ni7Pe",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' **Laminate Lumber Sawing Solutions**'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"A saw for laminate wood\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8792cV9FjL6C"
   },
   "source": [
    "### Sequential chain\n",
    "\n",
    "A sequential chain makes a series of calls to an LLM. It enables a pipeline-style workflow in which the output from one call becomes the input to the next.\n",
    "\n",
    "The two types include:\n",
    "\n",
    "* `SimpleSequentialChain`, where predictably each step has a single input and output, which becomes the input to the next step.\n",
    "\n",
    "* `SequentialChain`, which allows for multiple inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "cwKGeaiJi8uU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "25X7ALMEjPs-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an LLMChain to write a pitch for a new product\n",
    "# Let's increase the temperature to allow some imagination\n",
    "\n",
    "llm = VertexAI(temperature=0.7)\n",
    "template = \"\"\"You are an entrepreneur. Think of a ground breaking new product and write a short pitch.\n",
    "\n",
    "Title: {title}\n",
    "Entrepreneur: This is a pitch for the above product:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "pitch_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6m7H7JavjVsL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are a panelist on Dragon's Den. Given a \\\n",
    "description of the product, you are to explain why you think it will \\\n",
    "succeed or fail in the market.\n",
    "\n",
    "Product pitch: {pitch}\n",
    "Review by Dragon's Den panelist:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"pitch\"], template=template)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "RsYZq5BBjpFp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "overall_chain = SimpleSequentialChain(chains=[pitch_chain, review_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "lI2kt13mjsAx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m **Introducing the Portable Iced Coffee Maker: The Coolest Way to Brew Your Favorite Drink!**\n",
      "\n",
      "**Problem:** Iced coffee is a delicious and refreshing beverage, but it can be a hassle to make at home. Traditional methods require a lot of time and effort, and they often result in watered-down coffee.\n",
      "\n",
      "**Solution:** The Portable Iced Coffee Maker is the perfect solution for coffee lovers who want to enjoy delicious iced coffee without all the hassle. This revolutionary new product brews iced coffee in just minutes, and it's so portable that you can take it with you wherever you go.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Quick and\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m As a panelist on Dragon's Den, I would have several concerns about the Portable Iced Coffee Maker. While the idea of a portable iced coffee maker is appealing, there are several factors that could limit its success in the market.\n",
      "\n",
      "**Competition:** The market for portable coffee makers is already quite crowded, with several established brands offering similar products. To succeed, the Portable Iced Coffee Maker would need to offer a compelling unique selling proposition (USP) that sets it apart from the competition.\n",
      "\n",
      "**Functionality:** The success of the Portable Iced Coffee Maker would hinge on its ability to brew high-quality iced coffee quickly and efficiently. If the\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Portable iced coffee maker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7LkrG1KjxRv"
   },
   "source": [
    "### Router chain\n",
    "\n",
    "A `RouterChain` dynamically selects the next chain to use for a given input.\n",
    "This feature uses the `MultiPromptChain` to select then answer with the best-suited prompt to the question.\n",
    "\n",
    "This can help a modular architecure, allowing the effective triaging of inputs between relevant prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "6esOD8KCjtY0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "korean_template = \"\"\"\n",
    "You are an expert in korean history and culture.\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "spanish_template = \"\"\"\n",
    "You are an expert in spanish history and culture.\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "chinese_template = \"\"\"\n",
    "You are an expert in Chinese history and culture.\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qqoJDRGkjzRL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"korean\",\n",
    "        \"description\": \"Good for answering questions about Korean history and culture\",\n",
    "        \"prompt_template\": korean_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"spanish\",\n",
    "        \"description\": \"Good for answering questions about Spanish history and culture\",\n",
    "        \"prompt_template\": spanish_template,\n",
    "    },\n",
    "     {\n",
    "        \"name\": \"chinese\",\n",
    "        \"description\": \"Good for answering questions about Chinese history and culture\",\n",
    "        \"prompt_template\": chinese_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "DalpVKISj3gl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "\n",
    "llm = VertexAI(temperature=0)\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zAKrHbR-kABO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "xZZ1fg0kkADv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thanks to Deeplearning.ai for this template and for the\n",
    "# Langchain short course at deeplearning.ai/short-courses/.\n",
    "\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "rlHKi0yMkAGS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "A-HucZwnkAII",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "                         destination_chains=destination_chains,\n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGKSiUOZkMo-"
   },
   "source": [
    "Notice in the outputs the country of speciality is prefixed eg:\n",
    "`chinese: {'input': ...`, denoting the routing to the correct expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "y6H3dVYEkHNw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "chinese: {'input': 'What was the Han Dynasty?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The Han Dynasty (206 BCE–220 CE) was a period of imperial rule in China that followed the fall of the Qin Dynasty. It is considered one of the most prosperous and influential periods in Chinese history. The Han Dynasty was founded by Liu Bang, a peasant rebel who overthrew the Qin Dynasty. The Han Dynasty was divided into two periods: the Western Han Dynasty (206 BCE–9 CE) and the Eastern Han Dynasty (25–220 CE). The Western Han Dynasty was based in Chang'an (modern Xi'an), while the Eastern Han Dynasty was based in Luoyang.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What was the Han Dynasty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "AMUtRs_BkRG5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "spanish: {'input': 'What are some of typical dishes in Catalonia?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" **Catalan cuisine** is a Mediterranean cuisine that has been influenced by the region's history and geography. Some of the most typical dishes in Catalonia include:\\n\\n* **Pa amb tomàquet** (bread with tomato): This is a simple but delicious dish that is often served as a snack or appetizer. It is made with toasted bread that is rubbed with tomato and drizzled with olive oil.\\n* **Esqueixada** (cod salad): This salad is made with shredded cod, tomatoes, onions, and peppers. It is often served with bread or crackers.\\n* **Escudella i carn d'olla**\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What are some of typical dishes in Catalonia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "4ZZFI5NWkS-Z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "korean: {'input': \"How would I greet a friend's parents in Korean?\"}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" To greet a friend's parents in Korean, you would use the following phrases:\\n\\n**For the father:**\\n\\n- 안녕하세요 (annyeonghaseyo): This is the most common way to greet someone in Korean. It can be used in both formal and informal settings.\\n- 아저씨 (ajeossi): This is a respectful way to address an older man. It can be used for your friend's father or any other older man you meet.\\n- 아버지 (abeoji): This is a more formal way to address your friend's father. It is typically used when you are meeting\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"How would I greet a friend's parents in Korean?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "kcF_GMsvkU91",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "spanish: {'input': 'Summarize Don Quixote in a short paragraph'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Don Quixote, written by Miguel de Cervantes Saavedra, is a classic Spanish novel that tells the story of Alonso Quijano, a hidalgo from La Mancha who reads so many chivalric romances that he loses his mind and decides to become a knight-errant, calling himself Don Quixote of La Mancha. He recruits a peasant, Sancho Panza, as his squire, and together they embark on a series of adventures, often mistaking everyday objects and people for enchanted beings and formidable enemies. Through their misadventures, Cervantes satirizes the chivalric romances popular at the time and explores themes of idealism, reality, and the nature'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Summarize Don Quixote in a short paragraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "BudujtydkWUM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'How can I fix a carburetor?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Assistant: Fixing a carburetor can be a complex task, but here are some general steps you can follow to troubleshoot and potentially fix a carburetor issue:\\n\\n1. **Identify the Problem**:\\n   - Check for any visible signs of damage or leaks on the carburetor.\\n   - Listen for any unusual noises or changes in engine performance.\\n   - Look for any disconnected or loose hoses or cables.\\n\\n2. **Clean the Carburetor**:\\n   - Remove the carburetor from the engine.\\n   - Disassemble the carburetor and clean all the components thoroughly using a carburetor cleaner and a soft brush.\\n   - Pay special attention'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No specialist chain for carburetor advice; this\n",
    "# will be handled as any other input by the foundational model\n",
    "chain.run(\"How can I fix a carburetor?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXe_R6V_kf7G"
   },
   "source": [
    "## Agents and vectorstores\n",
    "\n",
    "This final section of the notebook will cover some of LangChain's most fun and powerful features.\n",
    "\n",
    "Agents have access to tools such as JSON, Wikipedia, Web Search, GitHub or Pandas Dataframes, and can access their capabilities depending on user input.\n",
    "\n",
    "See [here](https://python.langchain.com/docs/integrations/toolkits/) for a full list of agent toolkits.\n",
    "\n",
    "We will work with some data to perform data retrieval using the LLM with embeddings to match customer queries to products. This is known as Retrieval Augmentated Generation, or RAG.\n",
    "\n",
    "We will use the Wayfair [WANDS](https://www.aboutwayfair.com/careers/tech-blog/wayfair-releases-wands-the-largest-and-richest-publicly-available-dataset-for-e-commerce-product-search-relevance) dataset of more than 42,000 products. Here are the steps:\n",
    "\n",
    "* Download the data into a pandas dataframe and take a smaller 1,000-row sample set\n",
    "\n",
    "* Merge then generate embeddings for the product titles and descriptions\n",
    "\n",
    "* Prompt an LLM to retrieve details and relevant documents related to queries.\n",
    "\n",
    "<img src=\"https://assets.wfcdn.com/im/01139917/resize-h800-w800%5Ecompr-r85/2315/231567967/Capricornus+3+Seater+Sofa.jpg\" width=\"250\"/> <img src=\"https://assets.wfcdn.com/im/07725066/resize-h800-w800%5Ecompr-r85/1584/158440119/Vancasso+BOMOOTIUR+Stoneware+Dinnerware+-+Set+of+18.jpg\" width=\"250\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "MCr6_IMGlUoq",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723293532.171223    7462 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/wayfair/WANDS/main/dataset/product.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "qY2cTO5ilY1E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "product_df = pd.read_csv(\"product.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9b6WXptlcsv"
   },
   "source": [
    "We will work with 1,000 items to avoid longer wait times for the embedding and look up processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Ndlo2oNONYxj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_df = product_df[:2000].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "1ihFzIFAlajw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_class</th>\n",
       "      <th>category hierarchy</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_features</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>solid wood platform bed</td>\n",
       "      <td>Beds</td>\n",
       "      <td>Furniture / Bedroom Furniture / Beds &amp; Headboa...</td>\n",
       "      <td>good , deep sleep can be quite difficult to ha...</td>\n",
       "      <td>overallwidth-sidetoside:64.7|dsprimaryproducts...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>all-clad 7 qt . slow cooker</td>\n",
       "      <td>Slow Cookers</td>\n",
       "      <td>Kitchen &amp; Tabletop / Small Kitchen Appliances ...</td>\n",
       "      <td>create delicious slow-cooked meals , from tend...</td>\n",
       "      <td>capacityquarts:7|producttype : slow cooker|pro...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>all-clad electrics 6.5 qt . slow cooker</td>\n",
       "      <td>Slow Cookers</td>\n",
       "      <td>Kitchen &amp; Tabletop / Small Kitchen Appliances ...</td>\n",
       "      <td>prepare home-cooked meals on any schedule with...</td>\n",
       "      <td>features : keep warm setting|capacityquarts:6....</td>\n",
       "      <td>208.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                             product_name product_class  \\\n",
       "0           0                  solid wood platform bed          Beds   \n",
       "1           1              all-clad 7 qt . slow cooker  Slow Cookers   \n",
       "2           2  all-clad electrics 6.5 qt . slow cooker  Slow Cookers   \n",
       "\n",
       "                                  category hierarchy  \\\n",
       "0  Furniture / Bedroom Furniture / Beds & Headboa...   \n",
       "1  Kitchen & Tabletop / Small Kitchen Appliances ...   \n",
       "2  Kitchen & Tabletop / Small Kitchen Appliances ...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  good , deep sleep can be quite difficult to ha...   \n",
       "1  create delicious slow-cooked meals , from tend...   \n",
       "2  prepare home-cooked meals on any schedule with...   \n",
       "\n",
       "                                    product_features  rating_count  \\\n",
       "0  overallwidth-sidetoside:64.7|dsprimaryproducts...          15.0   \n",
       "1  capacityquarts:7|producttype : slow cooker|pro...         100.0   \n",
       "2  features : keep warm setting|capacityquarts:6....         208.0   \n",
       "\n",
       "   average_rating  review_count  \n",
       "0             4.5          15.0  \n",
       "1             2.0          98.0  \n",
       "2             3.0         181.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "9w5fj9adNsUM",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1269"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "xtft6ycVlqbk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce the df to columns of interest\n",
    "product_df = product_df.filter([\"product_id\", \"product_name\", \"product_description\", \"average_rating\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "oQBJPTihlsb9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_description</th>\n",
       "      <th>average_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>solid wood platform bed</td>\n",
       "      <td>good , deep sleep can be quite difficult to ha...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>all-clad 7 qt . slow cooker</td>\n",
       "      <td>create delicious slow-cooked meals , from tend...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>all-clad electrics 6.5 qt . slow cooker</td>\n",
       "      <td>prepare home-cooked meals on any schedule with...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                             product_name  \\\n",
       "0           0                  solid wood platform bed   \n",
       "1           1              all-clad 7 qt . slow cooker   \n",
       "2           2  all-clad electrics 6.5 qt . slow cooker   \n",
       "\n",
       "                                 product_description  average_rating  \n",
       "0  good , deep sleep can be quite difficult to ha...             4.5  \n",
       "1  create delicious slow-cooked meals , from tend...             2.0  \n",
       "2  prepare home-cooked meals on any schedule with...             3.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiHIRcVsl0F-"
   },
   "source": [
    "### Import and initialize pandas dataframe agent\n",
    "\n",
    "These tools use the `langchain-experimental` pip package installed at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "carYxcSOm_WL"
   },
   "source": [
    "### Pandas agent\n",
    "\n",
    "This agent allows us to interact with the dataframe using natural language. LangChain shows us the pandas queries it is composing to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "xP8sJ5TqlzeW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "agent = create_pandas_dataframe_agent(VertexAI(temperature=0), product_df, verbose=True, allow_dangerous_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "pDjIU-l8l2N_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m Question: how many rows are there?\n",
      "Thought: The number of rows in a dataframe can be found using the `len()` function.\n",
      "Action: python_repl_ast\n",
      "Action Input: `len(df)`\u001b[0m\u001b[36;1m\u001b[1;3m1269\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "HB3GlVLjmsgi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `df[df['average_rating'] > 4]`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `df[df['average_rating'] > 4]`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `df[df['average_rating'] > 4].shape[0]`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `df[df['average_rating'] > 4].shape[0]`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `df[df['average_rating'] > 4].shape[0]`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `df[df['average_rating'] > 4].shape[0]`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: How many products have a rating of > 4?\n",
      "Thought: I need to filter the dataframe to only include products with a rating greater than 4.\n",
      "Action: [python_repl_ast]\n",
      "Action Input: `len(df[df['average_rating'] > 4])`\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many products have a rating of > 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8A5C_jQm4UU"
   },
   "source": [
    "### CSV agent\n",
    "\n",
    "We can also work directly on a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "LDxCGtBCnO4M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(product_df, \"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "U-BV7NdvmueF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "\n",
    "agent = create_csv_agent(\n",
    "    VertexAI(temperature=0),\n",
    "    \"data.csv\",\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    allow_dangerous_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "YnrlNtJemx7J",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The shape of the dataframe should give me the number of rows.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape\u001b[0m\u001b[36;1m\u001b[1;3m(1269, 5)\u001b[0m\u001b[32;1m\u001b[1;3m The first value in the tuple is the number of rows.\n",
      "Final Answer: 1269\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1269'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "id": "B8GeRRuynd2T",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` Thought: The question asks for product descriptions that mention cedar wood. I can use `df['product_description'].str.contains('cedar wood')` to find the rows that contain 'cedar wood' in the 'product_description' column. Then I can use `df[df['product_description'].str.contains('cedar wood')]` to filter the dataframe and only keep the rows that contain 'cedar wood' in the 'product_description' column. Finally, I can use `df.to_json(orient='records')` to convert the filtered dataframe to JSON format.\nAction: python_repl_`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1346\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1346\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:463\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3254\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:2054\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 2054\u001b[0m     chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:3211\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[0;32m-> 3211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:855\u001b[0m, in \u001b[0;36mRunnable.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03mDefault implementation of stream, which calls invoke.\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;124;03mSubclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    The output of the Runnable.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:192\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1784\u001b[0m         Output,\n\u001b[0;32m-> 1785\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/config.py:427\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:193\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 193\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    195\u001b[0m         config,\n\u001b[1;32m    196\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:237\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mThe return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    Structured output.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/output_parsers/react_single_input.py:84\u001b[0m, in \u001b[0;36mReActSingleInputOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[1;32m     83\u001b[0m ):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\n\u001b[1;32m     87\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     88\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: ` Thought: The question asks for product descriptions that mention cedar wood. I can use `df['product_description'].str.contains('cedar wood')` to find the rows that contain 'cedar wood' in the 'product_description' column. Then I can use `df[df['product_description'].str.contains('cedar wood')]` to filter the dataframe and only keep the rows that contain 'cedar wood' in the 'product_description' column. Finally, I can use `df.to_json(orient='records')` to convert the filtered dataframe to JSON format.\nAction: python_repl_`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo any product descriptions mention cedar wood? Output them as JSON\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    606\u001b[0m         _output_key\n\u001b[1;32m    607\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    381\u001b[0m }\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1612\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1612\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1621\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1622\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1318\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1311\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1318\u001b[0m         [\n\u001b[1;32m   1319\u001b[0m             a\n\u001b[1;32m   1320\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1321\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1322\u001b[0m                 color_mapping,\n\u001b[1;32m   1323\u001b[0m                 inputs,\n\u001b[1;32m   1324\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1325\u001b[0m                 run_manager,\n\u001b[1;32m   1326\u001b[0m             )\n\u001b[1;32m   1327\u001b[0m         ]\n\u001b[1;32m   1328\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1318\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1311\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1318\u001b[0m         [\n\u001b[1;32m   1319\u001b[0m             a\n\u001b[1;32m   1320\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1321\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1322\u001b[0m                 color_mapping,\n\u001b[1;32m   1323\u001b[0m                 inputs,\n\u001b[1;32m   1324\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1325\u001b[0m                 run_manager,\n\u001b[1;32m   1326\u001b[0m             )\n\u001b[1;32m   1327\u001b[0m         ]\n\u001b[1;32m   1328\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1357\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m-> 1357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1362\u001b[0m     )\n\u001b[1;32m   1363\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` Thought: The question asks for product descriptions that mention cedar wood. I can use `df['product_description'].str.contains('cedar wood')` to find the rows that contain 'cedar wood' in the 'product_description' column. Then I can use `df[df['product_description'].str.contains('cedar wood')]` to filter the dataframe and only keep the rows that contain 'cedar wood' in the 'product_description' column. Finally, I can use `df.to_json(orient='records')` to convert the filtered dataframe to JSON format.\nAction: python_repl_`"
     ]
    }
   ],
   "source": [
    "agent.run(\"Do any product descriptions mention cedar wood? Output them as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Px8rsmWWng9m",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average_rating'].apply(lambda x: x ** 0.5)\n",
      "\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas?\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m Question: What is the square root of all ratings for product names featuring sofas\n",
      "Thought: The question asks for the square root of all ratings for product names featuring sofas. To answer this question, I need to first filter the dataframe to only include rows where the product name contains the word \"sofa\", then calculate the square root of the average_rating column for those rows.\n",
      "Action: [python_repl_ast]\n",
      "Action Input:\n",
      "```python\n",
      "sofa_df = df[df['product_name'].str.contains('sofa')]\n",
      "sofa_df['average_rating_sqrt'] = sofa_df['average\u001b[0m[python_repl_ast] is not a valid tool, try one of [python_repl_ast].\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the square root of all ratings for product names featuring sofas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AhOEC8snqs1"
   },
   "source": [
    "### Vector stores\n",
    "\n",
    "We will explore embeddings vectors and vector stores in more detail in [subsequent notebooks](rastringer.io.github.com/promptcraft). Let's see what's possible by concatenating our `product_title` and `product_description` columns and creating a text file from the result. We can then create embeddings and perform various retrieval and Q&A tasks.\n",
    "\n",
    "We will use the open source [Chroma](https://docs.trychroma.com/) vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "X_afza56nyru",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6OAprgzPMXu"
   },
   "source": [
    "We will embed a `text_data` column, which will be a concatenation of `product_name` and `product_description`, since both columns provide useful contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "FDQPwlsUnyt2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_df['text_data'] = product_df['product_name'] + \" \" + product_df['product_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "aookNZO4PJE0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       solid wood platform bed good , deep sleep can ...\n",
       "1       all-clad 7 qt . slow cooker create delicious s...\n",
       "2       all-clad electrics 6.5 qt . slow cooker prepar...\n",
       "3       all-clad all professional tools pizza cutter t...\n",
       "4       baldwin prestige alcott passage knob with roun...\n",
       "                              ...                        \n",
       "1991    2 '' gel memory foam mattress topper the refre...\n",
       "1993    0.5 '' polyester mattress pad this luxurious m...\n",
       "1995    alwyn home two-sided 6.5 '' firm memory foam m...\n",
       "1996    8 '' medium innerspring mattress if you want t...\n",
       "1999    betances all in one bed blocker hypoallergenic...\n",
       "Name: text_data, Length: 1269, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df[\"text_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "vcXg4duWnywK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the \"text_data\" column to a text file\n",
    "text_file_path = \"combined_text_data.txt\"\n",
    "product_df['text_data'].to_csv(text_file_path, sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "HgabFckIn4ML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"combined_text_data.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwLsDcG-Ph-t"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5__alNyWPiG4"
   },
   "source": [
    "### Text splitter\n",
    "\n",
    "Splitting text is common when working with LangChain and LLMs in general. This practice means we can feed large amounts of data to LLMs for parsing or embedding in chunks, or batches.\n",
    "\n",
    "Ideally, we want to do so in a way that keeps meaningful chunks together. We will use the default recommended `RecursiveCharacterTextSplitter`. We specify a `chunk_size` and `chunk_overlap` to set an upper limit on the size and overlap between the splits / chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "-w5bcOjgn5oc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "TUGo_V-Zn6yS",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "MBAQIkTEn8MP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723293698.155377    7462 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Clear any previous vector store\n",
    "!rm -rf ./docs/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZsonVgen9rk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-08-10 12:41:50.696189: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 12:41:58.329090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-08-10 12:41:58.330914: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-08-10 12:41:58.330942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad5b4d5141444778a7a42b17bf2cd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20999d05baae45c19752658af811b9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6725355397494fada29ec55674c195c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b439818c0b8a4ad1ba576a4ca30c18f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdc889dfea341a79d6ebff19622f6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953c646983444556b649cc96408ecb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ea122f32f14918b25ceff0c0dcf895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d5a686fcc64ac39be64004b3b61971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c250f3c594941d6a1b58534d0bf6d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a318824836054ef3a6b61ce1c042f690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd3d4315da941ddbc877ec2e63b11a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Takes ~3 mins to run\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLHE2R5kolYN"
   },
   "outputs": [],
   "source": [
    "query = \"Is there a slow cooker?\"\n",
    "docs = db.similarity_search(query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePkNsfD_olbH"
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhmN82xyn-pR"
   },
   "outputs": [],
   "source": [
    "query = \"Recommend a durable door mat\"\n",
    "docs = db.similarity_search(query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8uYSe_eRG9K"
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNN4-OPjop3Y"
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "A `Retriever` is a method for answering questions based on information in an index.\n",
    "\n",
    "Here, we use `RetrievalQA` this ability with a question and answering chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhaPqMI1opLR"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIhv6u72ovGo"
   },
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2LlRGVKopO_"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
    "If you don't know the answer, just say that you don't know, \\\n",
    "don't try to make up an answer. Use three sentences maximum. \\\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpTMb0zZozCX"
   },
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2rLBHDYozE0"
   },
   "outputs": [],
   "source": [
    "question = \"Can you recommend comfortable bed sheets?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5IFVCxbRNqa"
   },
   "outputs": [],
   "source": [
    "question = \"How about a Persian-style rug for my living room.\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHr7hwltRiMc"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this whirlwind tour of some of LangChain's features, we covered:\n",
    "\n",
    "* Memory\n",
    "* Chains\n",
    "* Agents\n",
    "* Vector stores\n",
    "\n",
    "LangChain is a fast-evolving project. To explore more features and keep up-to-date with developments, please see the [website](https://www.langchain.com/) or [Python documentation](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "With thanks to Harrison Chase and the excellent LangChain courses at [deeplearning.ai](https://deeplearning.ai/short-courses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
